import os
import subprocess
import sys
import time
from typing import List, Dict, Any
import re  # å¯¼å…¥ re æ¨¡å—ç”¨äºæ­£åˆ™è¡¨è¾¾å¼
from tools.log import create_logger
import signal
import datetime  # ç”¨äºç”Ÿæˆæ—¶é—´æˆ³
import glob  # ç”¨äºæŸ¥æ‰¾æ–‡ä»¶
import shutil  # ç”¨äºç§»åŠ¨æ–‡ä»¶

# --- æ ¸å¿ƒé…ç½®åŒº ---
# åœ¨è¿™é‡Œé›†ä¸­é…ç½®æ‰€æœ‰è¦è¿è¡Œçš„å®éªŒ
# æ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ä¸ªå®Œæ•´çš„å®éªŒæµç¨‹ï¼ˆç”Ÿæˆ + è¯„ä¼°ï¼‰

# --- å…¨å±€é»˜è®¤é…ç½® (å¦‚æœå®éªŒé…ç½®ä¸­æœªæŒ‡å®šï¼Œåˆ™ä½¿ç”¨æ­¤å¤„çš„é»˜è®¤å€¼) ---
DEFAULT_MODEL_CLS = "uni_qwen"
DEFAULT_CFG = 4.0
DEFAULT_BATCH_SIZE = 160
DEFAULT_NUM_GEN_REPEATS = 4
main_logger = create_logger("evaluation_all", './outputs/eval_all_using_template.log')

# --- æ–‡æœ¬è¯„æµ‹é…ç½® ---
TEXT_BATCH_SIZE = 64
NUM_FEW_SHOT = 0
TASKS = "mmlu,arc_challenge,arc_easy,boolq,winogrande"
# MODEL_ARGS ä¸­çš„ {ckpt} å°†è¢«åŠ¨æ€æ›¿æ¢ä¸ºæ¯ä¸ªå®éªŒçš„ model_path
MODEL_ARGS = 'pretrained={ckpt},model_type={model_type},architectures="[{arch}]"'
# Hugging Face é…ç½® (lm-eval ä¼šä½¿ç”¨)
HF_HOME = "../hf_cache"
HF_TOKEN = ""  # æ›¿æ¢ä¸º Hugging Face Token

# å›¾åƒç†è§£æµ‹è¯„é…ç½®
VIS_UND_TASKS = ["MMBench_DEV_EN", "MMBench_DEV_CN", "POPE", "MME"]
GEN_IMG_TASKS = ["dpg_bench", "geneval", "genai"]
SUPPORTED_TASKS = [*GEN_IMG_TASKS, "text", "vis_und"]

# --- ç¤ºä¾‹å®éªŒåˆ—è¡¨ ---
# æ³¨æ„ï¼š geneval, dpg éœ€è¦åœ¨ä¸åŒå®éªŒé‡Œ
EXPERIMENTS = [
    # åŸå§‹ Python è„šæœ¬ä¸­çš„ DPG-Bench è¯„ä¼° (ä¸­æ–‡)
    # {
    #     "name": "DPG-Bench Evaluation (zh)",
    #     "model_path": "../mock/ckpts/Qwen2.5-3B-uni-X/3B-unix-x12_6-try_sota-big-SFT-ignore_ins/20250912_2341/checkpoint-7800",
    #     "dpg_bench_prompts_path": "evaluation/T2I_Eval/dpg_bench/dpg_prompts_zh_fixed.jsonl",
    #     "eval_type": ["dpg_bench", "text"], # æŒ‡å®šè¯„ä¼°ç±»å‹ï¼šdpg_benchå›¾åƒè¯„ä¼° + textæ–‡æœ¬è¯„ä¼°
    #     # å…¶ä»–å‚æ•°å°†ä½¿ç”¨ä¸Šé¢çš„å…¨å±€é»˜è®¤å€¼
    # },
    # vis und
    # {
    #     "name": "Visual Understanding Evaluation",
    #     "model_path": "path/to/your/model",
    #     "eval_type": ["vis_und"],
    #     # å¯ä»¥è‡ªå®šä¹‰æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯å‚æ•°
    #     "vis_und_port": 33218,
    #     "vis_und_api_nproc": 80,
    # },
]

try:
    from evaluation.exp import EXPERIMENTS  # noqa
except Exception as e:
    print("éœ€è¦æ–°å»º exp.py å­˜æ”¾å®éªŒlist ")
    raise e
try:
    # for quick debug
    from evaluation.exp import VIS_UND_TASKS  # noqa

    print(f"å¼ºåˆ¶è®¾ç½®äº† VIS_UND TASKS {VIS_UND_TASKS}")
except:
    pass


def gen_save_path(model_path: str, exp_config: Dict[str, Any], eval_task) -> str:
    """
    æ ¹æ®æ¨¡å‹è·¯å¾„å’Œå®éªŒé…ç½®ï¼Œè‡ªåŠ¨ç”Ÿæˆå›¾åƒä¿å­˜è·¯å¾„ã€‚
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ¨¡å‹è·¯å¾„ä¸­è§£æå‡º model_tag å’Œ checkpoint_id
    # e.g., .../3B-unix-sft_v3/20250913_1654/checkpoint-5200
    match = re.search(r'([^/]+)/(\d{8}_\d{4})/?([^/]+)?$', model_path.rstrip('/'))
    if not match:
        # å¦‚æœæ­£åˆ™ä¸åŒ¹é…ï¼Œä½¿ç”¨ä¸€ä¸ªå¤‡ç”¨æ–¹æ¡ˆï¼Œç›´æ¥ç”¨æœ€åä¸€çº§ç›®å½•å
        model_name = os.path.basename(model_path.rstrip('/'))
    else:
        model_tag = match.group(1)
        ckpt_id = match.group(3)
        # ç»„åˆ model_tag å’Œ ckpt_id
        model_name = f"{model_tag}{'____' + ckpt_id if ckpt_id else ''}"

    # ç¡®å®šè¯„ä¼°ç±»å‹å¯¹åº”çš„ç»“æœæ–‡ä»¶å¤¹
    eval_folder_name = "unknown_results"
    if "dpg_bench" in eval_task:
        eval_folder_name = "dpg_results"
    elif "geneval" in eval_task:
        eval_folder_name = "geneval_results"
    elif "genai" in eval_task:
        eval_folder_name = "genai_results"
    else:
        raise ValueError(f"ä¸éœ€è¦ä¸ºtask {eval_task} ç”Ÿæˆä¿å­˜è·¯å¾„")

    # ä» prompts_path ä¸­æ¨æ–­è¯­è¨€
    lang = ''
    for k in exp_config:
        if 'prompts_path' in k:
            prompts_path = exp_config[k]
            if "zh" in prompts_path:
                lang = "zh"
            else:
                lang = 'en'
            break

    # è·å– CFG å’Œç”Ÿæˆæ¬¡æ•°é…ç½®
    cfg = exp_config.get('cfg', DEFAULT_CFG)
    num_repeats = exp_config.get('num_gen_repeats', DEFAULT_NUM_GEN_REPEATS)

    # ç»„è£…æœ€ç»ˆçš„æ–‡ä»¶å
    save_name = f"{model_name}_____{lang}_cfg{cfg}_{num_repeats}run"

    # è¿”å›å®Œæ•´çš„ä¿å­˜è·¯å¾„
    return os.path.join("evaluation", "T2I_Eval", eval_folder_name, save_name)


def run_command(command: List[str], env: dict = None, **kwargs):
    """è¾…åŠ©å‡½æ•°ï¼šè¿è¡Œä¸€ä¸ªå¤–éƒ¨å‘½ä»¤å¹¶æ£€æŸ¥å…¶æ˜¯å¦æˆåŠŸæ‰§è¡Œã€‚"""
    main_logger.info(f"ğŸš€ è¿è¡Œå‘½ä»¤: {' '.join(command)}")
    try:
        # ä½¿ç”¨ logger æ•è·å­è¿›ç¨‹çš„è¾“å‡º
        process = subprocess.Popen(command, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, **kwargs)
        for line in iter(process.stdout.readline, ''):
            main_logger.info(line.strip())
        process.wait()
        if process.returncode != 0:
            raise subprocess.CalledProcessError(process.returncode, command)
        main_logger.info("âœ… å‘½ä»¤æˆåŠŸå®Œæˆã€‚")
    except FileNotFoundError:
        main_logger.error(f"âŒ é”™è¯¯: å‘½ä»¤ '{command[0]}' æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿å®ƒå·²å®‰è£…å¹¶ä¸”åœ¨ç³»ç»Ÿçš„ PATH ä¸­ã€‚")
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        main_logger.error(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œé€€å‡ºç : {e.returncode}")
        sys.exit(1)
    except KeyboardInterrupt:
        main_logger.info("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­äº†è„šæœ¬ã€‚")
        sys.exit(1)


def run_generation(exp_config: Dict[str, Any], gpu_list: List[str]):
    """ä¸ºå•ä¸ªå®éªŒé…ç½®å¹¶è¡Œæ‰§è¡Œå›¾åƒç”Ÿæˆä»»åŠ¡ã€‚"""
    eval_types = exp_config.get('eval_type', [])
    image_eval_tasks = [t for t in eval_types if t in GEN_IMG_TASKS]

    if not image_eval_tasks:
        main_logger.info("â­ï¸  æœªé…ç½®å›¾åƒè¯„ä¼°ç±»å‹ï¼Œè·³è¿‡å›¾åƒç”Ÿæˆã€‚")
        return

    # å­˜å‚¨æ¯ä¸ªç”Ÿæˆä»»åŠ¡çš„ä¿å­˜è·¯å¾„ï¼Œä¾›åç»­è¯„ä¼°æ­¥éª¤ä½¿ç”¨
    if 'generated_paths' not in exp_config:
        exp_config['generated_paths'] = {}

    for task in image_eval_tasks:
        main_logger.info(f"\n{'=' * 20} å¼€å§‹ä¸ºè¯„ä¼°ç±»å‹ '{task}' ç”Ÿæˆå›¾åƒ {'=' * 20}")

        # ä¸ºå½“å‰ä»»åŠ¡åˆ›å»ºä¸´æ—¶é…ç½®
        task_config = exp_config.copy()
        task_config['eval_type'] = [task]

        chunks = len(gpu_list)
        model_path = task_config['model_path']
        save_path = gen_save_path(model_path, task_config, task)
        exp_config['generated_paths'][task] = save_path

        prompts_path = task_config[f'{task}_prompts_path']
        model_cls = task_config.get('model_cls', DEFAULT_MODEL_CLS)
        cfg_scale = task_config.get('cfg', DEFAULT_CFG)
        batch_size = task_config.get('batch_size', DEFAULT_BATCH_SIZE)
        grid_img = 1 if task == "dpg_bench" else 0
        num_gen_repeats = task_config.get('num_gen_repeats', DEFAULT_NUM_GEN_REPEATS)

        main_logger.info(f"ğŸ¨ å¼€å§‹ä¸ºå®éªŒ '{task_config.get('name', 'Untitled')}' [{task}] å¹¶è¡Œç”Ÿæˆå›¾åƒ...")
        main_logger.info(f"  - æ¨¡å‹è·¯å¾„: {model_path}")
        main_logger.info(f"  - ç»“æœä¿å­˜è·¯å¾„: {save_path}")
        main_logger.info("-" * 50)

        processes = []
        for idx in range(chunks):
            gpu_id = gpu_list[idx]
            env = os.environ.copy()
            env['PYTHONPATH'] = '.'
            env['CUDA_VISIBLE_DEVICES'] = gpu_id

            command = [
                sys.executable,
                "evaluation/T2I_Eval/gen_img.py",
                "--model_path", model_path,
                "--model_cls", model_cls,
                "--save_path", save_path,
                "--cfg_scale", str(cfg_scale),
                "--prompts_path", prompts_path,
                "--batch_size", str(batch_size),
                "--num_chunks", str(chunks),
                "--chunk_idx", str(idx),
                "--grid_img", str(grid_img),
                "--num_gen_repeats", str(num_gen_repeats),
            ]

            main_logger.info(f"  - å¯åŠ¨ä»»åŠ¡ {idx + 1}/{chunks} on GPU {gpu_id}...")
            # å°†å­è¿›ç¨‹çš„è¾“å‡ºé‡å®šå‘åˆ°æ—¥å¿—æ–‡ä»¶
            log_file = f"./outputs/gen_{task}_{idx}.log"
            proc = subprocess.Popen(command, env=env, stdout=open(log_file, 'w'), stderr=subprocess.STDOUT)
            processes.append(proc)

        main_logger.info(f"\nâ³ ç­‰å¾… '{task}' çš„æ‰€æœ‰å›¾åƒç”Ÿæˆä»»åŠ¡å®Œæˆï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
        for i, p in enumerate(processes):
            p.wait()
            if p.returncode != 0:
                main_logger.error(f"  - âŒ ç”Ÿæˆä»»åŠ¡ {i + 1}/{chunks} å¤±è´¥ï¼Œé€€å‡ºç : {p.returncode}ã€‚è¯¦æƒ…è¯·æŸ¥çœ‹ ./outputs/gen_{task}_{i}.log")
                sys.exit(1)
            else:
                main_logger.info(f"  - ä»»åŠ¡ {i + 1}/{chunks} å·²å®Œæˆã€‚")

        main_logger.info(f"\nâœ… '{task}' çš„å›¾åƒç”Ÿæˆä»»åŠ¡å‡å·²å®Œæˆï¼")


def run_gen_img_evaluation(exp_config: Dict[str, Any], gpu_list: List[str]):
    """æ ¹æ®å®éªŒé…ç½®æ‰§è¡Œç›¸åº”çš„å›¾åƒè¯„ä¼°ä»»åŠ¡ã€‚"""
    eval_types = exp_config.get('eval_type', [])
    if not eval_types:
        main_logger.info("â­ï¸ æœªæŒ‡å®š 'eval_type'ï¼Œè·³è¿‡è¯„ä¼°æ­¥éª¤ã€‚")
        return

    generated_paths = exp_config.get('generated_paths', {})
    num_gen_repeats = exp_config.get('num_gen_repeats', DEFAULT_NUM_GEN_REPEATS)
    chunks = len(gpu_list)
    env = os.environ.copy()
    env['PYTHONPATH'] = '.'

    for eval_type in eval_types:

        if eval_type not in GEN_IMG_TASKS:
            continue

        save_path = generated_paths[eval_type]

        path_info = f"'{save_path}'"
        main_logger.info(f"ğŸ“Š å¼€å§‹ä¸º {path_info} æ‰§è¡Œ '{eval_type}' è¯„ä¼°...")

        if eval_type == "dpg_bench":
            command = [
                "accelerate", "launch",
                "--num_processes", str(chunks),
                "--mixed_precision", 'fp16',
                "evaluation/T2I_Eval/dpg_bench/compute_dpg_bench.py",
                "--image-root-path", save_path,
                "--resolution", "512",
                "--pic-num", str(num_gen_repeats)
            ]
            main_logger.info(f"ğŸš€ åœ¨åå°å¯åŠ¨ dpg_bench è¯„ä¼°: {' '.join(command)}")
            subprocess.Popen(command, env=env)
            main_logger.info("âœ… dpg_bench å·²åœ¨åå°å¯åŠ¨ï¼Œè„šæœ¬å°†ç»§ç»­æ‰§è¡Œã€‚")

        elif eval_type == "geneval":
            command = ["bash", "evaluation/T2I_Eval/geneval/process.sh", save_path]
            run_command(command, env=env)

        elif eval_type == "genai":
            pass
            # Future-TODO
        else:
            raise NotImplementedError


def run_vis_und_evaluation(exp_config: Dict[str, Any], gpu_list: List[str]):
    """
    æ‰§è¡Œè§†è§‰ç†è§£è¯„ä¼°ï¼ŒåŒ…å«å¯åŠ¨APIæœåŠ¡å™¨å’Œè¿è¡Œè¯„ä¼°å®¢æˆ·ç«¯ã€‚
    æ­¤ç‰ˆæœ¬ç»è¿‡ä¿®æ”¹ï¼Œé€šè¿‡ç®¡ç†è¿›ç¨‹ç»„æ¥ç¡®ä¿èƒ½å¯é åœ°ç»ˆæ­¢APIæœåŠ¡å™¨åŠå…¶æ‰€æœ‰å­è¿›ç¨‹ã€‚
    """
    if "vis_und" not in exp_config.get("eval_type", []):
        return

    # å½’æ¡£ä¸Šä¸€æ¬¡çš„è¯„ä¼°ç»“æœ
    source_dir = "./outputs/my_custom_model"
    files_to_archive = glob.glob(os.path.join(source_dir, "my*"))
    if files_to_archive:
        # ä½¿ç”¨å½“å‰æ—¶é—´åˆ›å»ºå½’æ¡£æ–‡ä»¶å¤¹åç§°
        time_str = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        archive_dir = os.path.join(source_dir, f"archive_{time_str}")
        os.makedirs(archive_dir, exist_ok=True)
        main_logger.info(f"ğŸ“¦ æ‰¾åˆ°æ—§çš„è¯„ä¼°ç»“æœï¼Œæ­£åœ¨å½’æ¡£åˆ°: {archive_dir}")
        for f in files_to_archive:
            try:
                # ç§»åŠ¨æ–‡ä»¶åˆ°å½’æ¡£ç›®å½•
                shutil.move(f, archive_dir)
            except Exception as e:
                main_logger.warning(f"  - å½’æ¡£æ–‡ä»¶ {f} å¤±è´¥: {e}")
        main_logger.info("âœ… å½’æ¡£å®Œæˆã€‚")

    main_logger.info("ğŸ§  å¼€å§‹æ‰§è¡Œè§†è§‰ç†è§£è¯„ä¼°...")
    model_path = exp_config['model_path']
    server_gpu = ','.join(gpu_list)
    port = exp_config.get("vis_und_port", 33218)

    server_params = {
        "num-gpus": exp_config.get("vis_und_server_gpus", 1),
        "max-tokens": exp_config.get("vis_und_max_tokens", 3),
        "min-new-tokens": exp_config.get("vis_und_min_new_tokens", 1),
        "acceptable-tokens": exp_config.get("vis_und_acceptable_tokens", "all"),  # é»˜è®¤è¿˜æ˜¯æ”¯æŒå…¨éƒ¨tokens
        "translate": exp_config.get("vis_und_translate", '0,0'),
        "max-batch-size": exp_config.get("vis_und_max_batch_size", 80),
        "model-cls": exp_config.get("model_cls", DEFAULT_MODEL_CLS)
    }
    client_model_name = exp_config.get("vis_und_model_name", "my_custom_model")
    client_api_nproc = exp_config.get("vis_und_api_nproc", 80)
    judge_model = exp_config.get('vis_und_judge_model', 'exact_matching')

    server_env = os.environ.copy()
    server_env.update({
        'PYTHONPATH': '.', 'CUDA_VISIBLE_DEVICES': server_gpu,
        'SERVER_SLEEP': str(exp_config.get("vis_und_server_sleep", 0.5))
    })
    if "tr_glm_api_key" in exp_config:
        server_env['TR_GLM_API_KEY'] = exp_config["tr_glm_api_key"]

    server_command = [sys.executable, "evaluation/api_server.py", "--model-path", model_path, "--port", str(port)]
    for key, value in server_params.items():
        server_command.append(f"--{key}")
        server_command.append(f"{value}")
    main_logger.info(f"å¯åŠ¨æŒ‡ä»¤: {' '.join(server_command)}")

    server_process = None
    try:
        main_logger.info(f"ğŸš€ åœ¨ GPU {server_gpu} ä¸Šåå°å¯åŠ¨ API æœåŠ¡å™¨...")
        # ã€å…³é”®ä¿®æ”¹ 1ã€‘: ä½¿ç”¨ preexec_fn=os.setsid å°†æœåŠ¡å™¨è¿›ç¨‹æ”¾å…¥ä¸€ä¸ªæ–°çš„è¿›ç¨‹ç»„ã€‚
        # è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥å‘æ•´ä¸ªç»„ï¼ˆåŒ…æ‹¬æ‰€æœ‰å­™å­è¿›ç¨‹ï¼‰å‘é€ä¿¡å·ã€‚
        server_process = subprocess.Popen(
            server_command,
            env=server_env,
            preexec_fn=os.setsid
        )

        main_logger.info("â³ ç­‰å¾… 30 ç§’ä»¥ç¡®ä¿æœåŠ¡å™¨å®Œå…¨å¯åŠ¨...")
        time.sleep(30)

        # æ£€æŸ¥æœåŠ¡å™¨è¿›ç¨‹æ˜¯å¦åœ¨ç­‰å¾…æœŸé—´æ„å¤–é€€å‡º
        if server_process.poll() is not None:
            raise RuntimeError("API æœåŠ¡å™¨åœ¨å¯åŠ¨æœŸé—´æ„å¤–é€€å‡ºï¼Œè¯·æ£€æŸ¥å…¶æ—¥å¿—ã€‚")

        # --- è¿è¡Œè¯„ä¼°å®¢æˆ·ç«¯ ---
        # è®¾ç½®è°ƒç”¨APIå®¢æˆ·ç«¯æ‰€éœ€çš„ç¯å¢ƒå˜é‡
        client_env = os.environ.copy()
        client_env.update({
            'HF_HOME': HF_HOME, 'HF_TOKEN': HF_TOKEN,
            # 'OPENAI_API_KEY': "sk-none",  # æœ¬åœ°æœåŠ¡, API Keyä¸é‡è¦
            # 'OPENAI_API_BASE': f"http://localhost:{port}/v1"
        })

        # å¾ªç¯æ‰§è¡Œæ‰€æœ‰æŒ‡å®šçš„VQAè¯„æµ‹ä»»åŠ¡
        for task in VIS_UND_TASKS:
            main_logger.info(f"  - ğŸš€ å¼€å§‹VQAä»»åŠ¡: {task}")
            command = [
                sys.executable,
                "evaluation/eval_vqa.py",
                "--task", task,
                "--api_nproc", str(client_api_nproc),
                "--save_name", exp_config.get("name", "Untitled")
            ]
            run_command(command, env=client_env)
            main_logger.info(f"  - âœ… VQAä»»åŠ¡ '{task}' å®Œæˆ")

    finally:
        if server_process and server_process.poll() is None:
            pgid = os.getpgid(server_process.pid)
            main_logger.info(f"ğŸ›‘ æ­£åœ¨ç»ˆæ­¢ API æœåŠ¡å™¨è¿›ç¨‹ç»„ (PGID: {pgid})...")
            try:
                # ã€å…³é”®ä¿®æ”¹ 2ã€‘: ä½¿ç”¨ os.killpg å‘æ•´ä¸ªè¿›ç¨‹ç»„å‘é€ç»ˆæ­¢ä¿¡å· (SIGTERM)ã€‚
                os.killpg(pgid, signal.SIGTERM)

                # ç­‰å¾…ä¸»è¿›ç¨‹ï¼ˆå¯åŠ¨å™¨ï¼‰å“åº”ä¿¡å·å¹¶é€€å‡º
                server_process.wait(timeout=20)
                main_logger.info("âœ… API æœåŠ¡å™¨è¿›ç¨‹ç»„å·²æ­£å¸¸ç»ˆæ­¢ã€‚")
            except subprocess.TimeoutExpired:
                main_logger.warning(f"ğŸ›‘ æœåŠ¡å™¨è¿›ç¨‹ç»„æœªèƒ½æ­£å¸¸ç»ˆæ­¢ï¼Œå¼ºåˆ¶å…³é—­ (PGID: {pgid})...")
                # å¦‚æœ SIGTERM æ— æ•ˆï¼Œåˆ™å‘é€ SIGKILL å¼ºåˆ¶æ€æ­»æ•´ä¸ªè¿›ç¨‹ç»„
                os.killpg(pgid, signal.SIGKILL)
                main_logger.info("âœ… API æœåŠ¡å™¨è¿›ç¨‹ç»„å·²å¼ºåˆ¶å…³é—­ã€‚")
            except ProcessLookupError:
                # è¿™ç§æƒ…å†µå¯èƒ½å‘ç”Ÿï¼šåœ¨æˆ‘ä»¬æ£€æŸ¥ poll() å’Œæ‰§è¡Œ killpg() ä¹‹é—´ï¼Œè¿›ç¨‹å·²ç»è‡ªå·±é€€å‡ºäº†
                main_logger.warning("æœåŠ¡å™¨è¿›ç¨‹ç»„åœ¨å°è¯•ç»ˆæ­¢æ—¶å·²ä¸å­˜åœ¨ã€‚")

    main_logger.info("âœ… è§†è§‰ç†è§£è¯„ä¼°å®Œæˆã€‚")


def run_text_evaluation(exp_config: Dict[str, Any], gpu_list: List[str]):
    """ä¸ºå•ä¸ªå®éªŒé…ç½®æ‰§è¡Œ lm-eval æ–‡æœ¬è¯„ä¼°ã€‚"""
    if "text" not in exp_config.get("eval_type", []):
        return

    model_path = exp_config['model_path']
    num_processes = len(gpu_list)
    model_type = exp_config.get('model_type', 'qwen3')
    arch = exp_config.get('arch', 'Qwen3ForCausalLM')
    model_args_formatted = MODEL_ARGS.format(ckpt=model_path, model_type=model_type, arch=arch)

    main_logger.info(f"ğŸ“ å¼€å§‹ä¸ºå®éªŒ '{exp_config.get('name', 'Untitled')}' æ‰§è¡Œæ–‡æœ¬è¯„æµ‹...")
    main_logger.info(f"  - æ¨¡å‹è·¯å¾„: {model_path}")
    main_logger.info(f"  - è¯„æµ‹ä»»åŠ¡: {TASKS}")
    main_logger.info("-" * 50)

    env = os.environ.copy()
    env.update({'PYTHONPATH': '.', 'HF_HOME': HF_HOME, 'HF_TOKEN': HF_TOKEN})

    command = [
        "accelerate", "launch", "--num_processes", str(num_processes),
        "--main_process_port", "19763", "-m", "lm_eval", "--model", "hf",
        "--tasks", TASKS, "--batch_size", str(TEXT_BATCH_SIZE),
        "--trust_remote_code", "--num_fewshot", str(NUM_FEW_SHOT),
        "--model_args", model_args_formatted,
    ]
    run_command(command, env=env)


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    main_logger.info("ğŸ”„ æ­£åœ¨ä» Git ä»“åº“æ‹‰å–æœ€æ–°ä»£ç ...")
    run_command(["git", "pull"])
    main_logger.info("-" * 70)

    # æ ¡éªŒæ‰€æœ‰å®éªŒé…ç½®ä¸­çš„è¯„ä¼°ä»»åŠ¡æ˜¯å¦æœ‰æ•ˆ
    for exp in EXPERIMENTS:
        if "eval_type" in exp:
            for task in exp["eval_type"]:
                if task not in SUPPORTED_TASKS:
                    main_logger.error(f"âŒ å®éªŒ '{exp.get('name', 'æœªå‘½å')}' ä¸­åŒ…å«ä¸æ”¯æŒçš„è¯„ä¼°ä»»åŠ¡: '{task}'")
                    main_logger.error(f"   æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ä¸º: {SUPPORTED_TASKS}")
                    sys.exit(1)
    main_logger.info("âœ… æ‰€æœ‰å®éªŒé…ç½®ä¸­çš„è¯„ä¼°ä»»åŠ¡å‡æœ‰æ•ˆã€‚")

    gpu_list_str = os.environ.get('CUDA_VISIBLE_DEVICES', '0,1,2,3,4,5,6,7')
    gpu_list = gpu_list_str.split(',')
    main_logger.info(f"ğŸ–¥ï¸ æ£€æµ‹åˆ°å¯ç”¨ GPUs: {gpu_list_str} (å…± {len(gpu_list)} ä¸ª)")
    main_logger.info("-" * 70)

    total_experiments = len(EXPERIMENTS)
    for i, experiment_config in enumerate(EXPERIMENTS):
        exp_name = experiment_config.get('name', f'å®éªŒ #{i + 1}')
        main_logger.info(f"ğŸš€ğŸš€ğŸš€ å¼€å§‹æ‰§è¡Œå®éªŒ {i + 1}/{total_experiments}: {exp_name} ğŸš€ğŸš€ğŸš€")

        run_generation(experiment_config, gpu_list)
        main_logger.info("-" * 50)

        run_gen_img_evaluation(experiment_config, gpu_list)
        main_logger.info("-" * 50)

        run_text_evaluation(experiment_config, gpu_list)
        main_logger.info("-" * 50)

        run_vis_und_evaluation(experiment_config, gpu_list)
        main_logger.info("-" * 50)

    main_logger.info("ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰å®éªŒå‡å·²æ‰§è¡Œå®Œæ¯•ï¼ğŸ‰ğŸ‰ğŸ‰")


if __name__ == "__main__":
    main()